run_name: #run-name-here
platform: #platform-name-here
gpu_num: 32
image: nvcr.io/nvidia/nemo:22.09
env_variables:
  - key: PYTHONUNBUFFERED
    value: '1'
command: | 

  apt update
  apt install -y parallel

  # getting the vocab, merge files for the tokenizer
  wget <https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json>
  wget <https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt>

  seq 0 8 | parallel -u \\ 'CUDA_VISIBLE_DEVICES={} RANK=$(( $NODE_RANK * 8 + {} )) python3 /mnt/nvme/jobs/nemo/NeMo-113-release/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\
  --config-path=/mnt/nvme/jobs/nemo/ \\
  --config-name=megatron_40b \\
  model.data.data_prefix=[1.0,/your_dataset_path_here/] \\
  model.tokenizer.vocab_file=gpt2-vocab.json \\
  model.tokenizer.merge_file=gpt2-merges.txt \\
  model.optim.name="fused_adam" \\
  trainer.devices=1'